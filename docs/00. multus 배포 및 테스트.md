## Multus 소개

 multus는 여러개의 Network interface를 pods에 연결할 수 있게 해주는 kubernetes용 CNI Plugin이다.
 
지원되는 plugin은 bridge, ipvlan, macvlan, ptp, host-device, vlan이 있으며 자세한 내용은 (링크)[https://www.cni.dev/plugins/current/main/] 여기서 확인이 가능하다.

dpdk 공식 사이트에서 올라온 (자료)[https://www.dpdk.org/wp-content/uploads/sites/35/2020/11/DPDK-Container-Plug-ins-2019-1.pdf]에서는 SR-IOV, DPDK도 사용할 수 있다. 

## macvlan 소개
macvlan은 한개의 physical network interface에 여러개의 MAC 주소를 설정할 수 있다. macvlan은 고유한 MAC 주소를 가지고 있으므로, DHCP 서버를 이용하여 쉽게 IP 할당을 할 수 있다.

## Multus 설치
해당 작업은 master 노드 또는 kubernetes cluster를 제어할 수 있는 클라이언트에서 작업한다.

Kuberenetes Cluster 정보
```
kubectl get node -owide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
master01   Ready    control-plane,master   22h   v1.23.4   172.27.100.10    <none>        Ubuntu 20.04.2 LTS   5.4.0-65-generic   containerd://1.4.12
worker01   Ready    <none>                 22h   v1.23.4   172.27.100.101   <none>        Ubuntu 20.04.2 LTS   5.4.0-65-generic   containerd://1.4.12
worker02   Ready    <none>                 22h   v1.23.4   172.27.100.102   <none>        Ubuntu 20.04.2 LTS   5.4.0-65-generic   containerd://1.4.12

kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS      AGE
calico-kube-controllers-56fcbf9d6b-nqbr8   1/1     Running   0             7m23s
calico-node-5zfxb                          1/1     Running   0             7m23s
calico-node-6klcc                          1/1     Running   0             7m23s
calico-node-gvllk                          1/1     Running   0             7m23s
coredns-64897985d-dkvpk                    1/1     Running   0             22h
coredns-64897985d-z2pjg                    1/1     Running   0             22h
haproxy-master01                           1/1     Running   1 (16m ago)   22h
kube-apiserver-master01                    1/1     Running   1 (16m ago)   22h
kube-controller-manager-master01           1/1     Running   1 (16m ago)   22h
kube-proxy-5mfv7                           1/1     Running   1 (16m ago)   22h
kube-proxy-gwvxs                           1/1     Running   1 (16m ago)   22h
kube-proxy-w8j4b                           1/1     Running   1 (16m ago)   22h
kube-scheduler-master01                    1/1     Running   1 (16m ago)   22h
```

```

git clone https://github.com/k8snetworkplumbingwg/multus-cni.git && cd multus-cni
cat ./deployments/multus-daemonset-thick-plugin.yml | kubectl apply -f -

kubectl get pod -n kube-system -l app=multus
NAME                   READY   STATUS    RESTARTS   AGE
kube-multus-ds-dnd6z   1/1     Running   0          91s
kube-multus-ds-dz789   1/1     Running   0          91s
kube-multus-ds-xsnp9   1/1     Running   0          91s

```

Daemonset으로 multus를 배포하면 /etc/cni/net.d/ 디렉터리 밑에 '00-multus.conf' 파일과 'multus.d' 디렉터리와 이 디렉터리 밑에 'multus.kubeconfig' 파일이 자동으로 생성된다.

```
sudo cat /etc/cni/net.d/00-multus.conf | jq
{
  "capabilities": {
    "bandwidth": true,
    "portMappings": true
  },
  "cniVersion": "0.3.1",
  "delegates": [
    {
      "cniVersion": "0.3.1",
      "name": "k8s-pod-network",
      "plugins": [
        {
          "datastore_type": "kubernetes",
          "ipam": {
            "type": "calico-ipam"
          },
          "kubernetes": {
            "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
          },
          "log_file_path": "/var/log/calico/cni/cni.log",
          "log_level": "info",
          "mtu": 0,
          "nodename": "master01",
          "policy": {
            "type": "k8s"
          },
          "type": "calico"
        },
        {
          "capabilities": {
            "portMappings": true
          },
          "snat": true,
          "type": "portmap"
        },
        {
          "capabilities": {
            "bandwidth": true
          },
          "type": "bandwidth"
        }
      ]
    }
  ],
  "logLevel": "verbose",
  "logToStderr": true,
  "kubeconfig": "/etc/cni/net.d/multus.d/multus.kubeconfig",
  "name": "multus-cni-network",
  "type": "multus"
}

```

이제 multus를 배포했으니, pod에 추가적으로 연결 될 NIC(Network Interface Card)를 정의한다.

위 multus 배포 과정에서 multus 관련 CRD(Custom Resource Define)가 생성 된다.

```
kubectl get crd network-attachment-definitions.k8s.cni.cncf.io
NAME                                             CREATED AT
network-attachment-definitions.k8s.cni.cncf.io   2022-03-04T14:45:48Z

```

해당 CRD의 일부 내용을 보연 아래와 같은 내용이 포함되어있다.
```
spec:
  conversion:
    strategy: None
  group: k8s.cni.cncf.io
  names:
    kind: NetworkAttachmentDefinition
    listKind: NetworkAttachmentDefinitionList
    plural: network-attachment-definitions
    shortNames:
    - net-attach-def
    singular: network-attachment-definition

```

kubernetes cluster에서 network-attachment-definitions의 api resources를 확인할 수 있다.

대략적으로 이 리소스을 이용하여 pod에 추가 NIC를 추가할 수 있다고 판단할 수 있겠다.
```
kubectl api-resources | grep net-attach-def
network-attachment-definitions    net-attach-def   k8s.cni.cncf.io/v1                     true         NetworkAttachmentDefinition

```

## macvlan 설정

macvlan 설정을 CR(Custom Resource)로 저장할 수 있다.


macvlan 설정 시 ```config``` 구간에 해당 랩에서 Private zone으로 명시된 IP 대역을 사용해야 한다.
주의해야할 점은 ```config``` 구간의 ```master``` 파라미터는 kubernetes cluster에 있는 ```호스트의 NIC명```과 일치해야 한다.

```metadata``` 밑에 ```name``` 필드는 pod에 macvlan 설정 명을 지정하여 사용할 수 있게한다.
```
cat <<EOF | kubectl create -f -
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: privatezone-macvlan-conf
spec:
  config: '{
      "cniVersion": "0.3.0",
      "type": "macvlan",
      "master": "enp0s8",
      "mode": "bridge",
      "ipam": {
        "type": "host-local",
        "subnet": "10.12.34.0/24",
        "rangeStart": "10.12.34.200",
        "rangeEnd": "10.12.34.220",
        "routes": [
          { "dst": "0.0.0.0/0" }
        ],
        "gateway": "10.12.34.254"
      }
    }'
EOF
```

배포 확인!
```
kubectl get net-attach-def
NAME                       AGE
privatezone-macvlan-conf   28s

 kubectl describe net-attach-def privatezone-macvlan-conf
Name:         privatezone-macvlan-conf
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2022-03-04T15:52:32Z
  Generation:          1
  Managed Fields:
    API Version:  k8s.cni.cncf.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        .:
        f:config:
    Manager:         kubectl-create
    Operation:       Update
    Time:            2022-03-04T15:52:32Z
  Resource Version:  13153
  UID:               4d9e82f8-89d6-4f74-a9ed-715b794b3d0f
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "macvlan", "master": "enp0s8", "mode": "bridge", "ipam": { "type": "host-local", "subnet": "10.12.34.0/24", "rangeStart": "10.12.34.200", "rangeEnd": "10.12.34.220", "routes": [ { "dst": "0.0.0.0/0" } ], "gateway": "10.12.34.254" } }
Events:    <none>

```

이제 위 과정에서 생성된 macvlan 정보를 pod에 설정하여 추가 NIC가 생성되는지 확인해보자.

아래 pod resource를 보면 Annotaion에 ```k8s.v1.cni.cncf.io/networks``` 필드가 있는데 해당 필드를 이용해 위에서 생성한 macvlan을 설정 할 수 있다.
```k8s.v1.cni.cncf.io/networks``` 필드의 값은 ```network-attachment-definitions``` 리소스에 있는 ```name```으로 해야한다.

```
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: net-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: privatezone-macvlan-conf
spec:
  containers:
  - name: netshoot-pod
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF
```
```
kubectl get pod -owide
NAME      READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
net-pod   1/1     Running   0          51s   10.244.5.2   worker01   <none>           <none>

kubectl describe pod net-pod

Name:         net-pod
Namespace:    default
Priority:     0
Node:         worker01/172.27.100.101
Start Time:   Sat, 05 Mar 2022 01:27:02 +0900
Labels:       <none>
Annotations:  cni.projectcalico.org/containerID: 79109d193f45aa16f825c0b2327b1bb8a4802ebb08995fc8429f4ef90fe62898
              cni.projectcalico.org/podIP: 10.244.5.2/32
              cni.projectcalico.org/podIPs: 10.244.5.2/32
              k8s.v1.cni.cncf.io/network-status:
                [{
                    "name": "k8s-pod-network",
                    "ips": [
                        "10.244.5.2"
                    ],
                    "default": true,
                    "dns": {}
                },{
                    "name": "default/privatezone-macvlan-conf",
                    "interface": "net1",
                    "ips": [
                        "10.12.34.201"
                    ],
                    "mac": "2e:c2:b8:fe:13:6f",
                    "dns": {}
                }]
              k8s.v1.cni.cncf.io/networks: privatezone-macvlan-conf
              k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "k8s-pod-network",
                    "ips": [
                        "10.244.5.2"
                    ],
                    "default": true,
                    "dns": {}
                },{
                    "name": "default/privatezone-macvlan-conf",
                    "interface": "net1",
                    "ips": [
                        "10.12.34.201"
                    ],
                    "mac": "2e:c2:b8:fe:13:6f",
                    "dns": {}
                }]
Events:
  Type    Reason          Age   From               Message
  ----    ------          ----  ----               -------
  Normal  Scheduled       65s   default-scheduler  Successfully assigned default/net-pod to worker01
  Normal  AddedInterface  66s   multus             Add eth0 [10.244.5.2/32] from k8s-pod-network
  Normal  AddedInterface  66s   multus             Add net1 [10.12.34.201/24] from default/privatezone-macvlan-conf
```


생성된 pod의 정보를 보면 eth0 외에 net1 NIC가 새로 추가되었으며, 위에서 설정한 macvlan의 IP 대역으로 배포되었다.

```
kubectl exec -it net-pod -- bash
bash-5.1# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default
    link/ether e6:66:1f:0b:18:a5 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.5.2/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::e466:1fff:fe0b:18a5/64 scope link
       valid_lft forever preferred_lft forever
5: net1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 2e:c2:b8:fe:13:6f brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.12.34.201/24 brd 10.12.34.255 scope global net1
       valid_lft forever preferred_lft forever
    inet6 fe80::2cc2:b8ff:fefe:136f/64 scope link
       valid_lft forever preferred_lft forever
  
```

해당 랩에서는 public zone과 private zone이 있는데, private zone 즉, 해당 pod의 net1은 인터넷 통신이 되지 않아야 한다.

ping 실행 시 eth0 NIC에서 외부 통신이 정상적으로 된다.
```
bash-5.1# ping -c 1 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=110 time=45.3 ms

--- 8.8.8.8 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 45.279/45.279/45.279/0.000 ms
```

net1 NIC에서는 외부 통신이 안된다.
```
bash-5.1# ping -I net1 -c 1 8.8.8.8
PING 8.8.8.8 (8.8.8.8) from 10.12.34.201 net1: 56(84) bytes of data.

--- 8.8.8.8 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms

```

근데 여기서 짚고 넘어 가야할 부분은 방화벽 단에서 Source NAT 설정을 안해서 외부 Out bound로 못 나가는건지, 아니면 호스트에서 패킷을 DROp 하는지 확인을 해봐야한다.

```
bash-5.1# ping -I net1 -c 1 10.12.34.254
PING 10.12.34.254 (10.12.34.254) from 10.12.34.201 net1: 56(84) bytes of data.

--- 10.12.34.254 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms

```
gateway로도 ping이 안간다.

Docker 공식문에서 (macvlan)[https://docs.docker.com/network/macvlan/] 해당 내용을 확인하면, "Your networking equipment needs to be able to handle “promiscuous mode”, where one physical interface can be assigned multiple MAC addresses" 이런 내용이 있다. 

호스트의 NIC에 여러개의 MAC 주소가 있을 경우 해당 NIC에 'promiscuous mode'를 활성화해야 한다는 것이다.

단순히 호스트에서 ```ip``` 명령어로 promiscuous mode'를 활성화 할 수 있지만, 재부팅 시 해당 설정은 초기화 되니 호스트가 재부팅 되어도 해당 설정이 유지될 수 있게 해줘야한다.

(만약 가상머신에서 테스트할 경우 가상머신 자체에서도 무작위 모드를 활성화 해줘야한다.)

```
cat <<EOF | sudo tee /etc/systemd/system/bridge-promisc.service
[Unit]
Description=Makes interfaces run in promiscuous mode at boot
After=network-online.target

[Service]
Type=oneshot
ExecStart=/usr/sbin/ip link set dev enp0s8 promisc on
TimeoutStartSec=0
RemainAfterExit=yes

[Install]
WantedBy=default.target
EOF
sudo systemctl enable bridge-promisc --now
```

이렇게 promiscuous mode가 활성화 된 것을 확인할 수 있다.
```
ip addr show dev  enp0s8
3: enp0s8: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:82:1a:95 brd ff:ff:ff:ff:ff:ff
    inet 10.12.34.10/24 brd 10.12.34.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fe82:1a95/64 scope link
       valid_lft forever preferred_lft forever
```

다시 pod에서 privatezone의 gateway로 ICMP 통신이 되는 것을 확인할 수 있다.
```
kubectl exec -it net-pod -- bash
bash-5.1# ping -I net1 -c 1 10.12.34.254
PING 10.12.34.254 (10.12.34.254) from 10.12.34.205 net1: 56(84) bytes of data.
64 bytes from 10.12.34.254: icmp_seq=1 ttl=64 time=0.203 ms

--- 10.12.34.254 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.203/0.203/0.203/0.000 ms

```

해당 pod는 worker01 노드에 올라가 있어 worker01 IP인 10.12.34.101 를 제외한 나머지는 ICMP 통신이 정상적으로 된다.

```
bash-5.1# ip route
default via 169.254.1.1 dev eth0
10.12.34.0/24 dev net1 proto kernel scope link src 10.12.34.205
169.254.1.1 dev eth0 scope link
bash-5.1# arp -a
? (172.27.100.101) at ee:ee:ee:ee:ee:ee [ether]  on eth0
? (169.254.1.1) at ee:ee:ee:ee:ee:ee [ether]  on eth0
? (10.12.34.254) at 08:00:27:a7:59:11 [ether]  on net1

bash-5.1# ping -I net1  -c 1 10.12.34.10
PING 10.12.34.10 (10.12.34.10) from 10.12.34.205 net1: 56(84) bytes of data.
64 bytes from 10.12.34.10: icmp_seq=1 ttl=64 time=0.175 ms

--- 10.12.34.10 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.175/0.175/0.175/0.000 ms
bash-5.1# ping -I net1 -c 1 10.12.34.10
PING 10.12.34.10 (10.12.34.10) from 10.12.34.205 net1: 56(84) bytes of data.
64 bytes from 10.12.34.10: icmp_seq=1 ttl=64 time=0.206 ms

--- 10.12.34.10 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.206/0.206/0.206/0.000 ms
bash-5.1# ping -I net1 -c 1 10.12.34.101
PING 10.12.34.101 (10.12.34.101) from 10.12.34.205 net1: 56(84) bytes of data.

--- 10.12.34.101 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms

bash-5.1# ping -I net1 -c 1 10.12.34.102
PING 10.12.34.102 (10.12.34.102) from 10.12.34.205 net1: 56(84) bytes of data.
64 bytes from 10.12.34.102: icmp_seq=1 ttl=64 time=0.186 ms

--- 10.12.34.102 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.186/0.186/0.186/0.000 ms
bash-5.1# arp -a
? (172.27.100.101) at ee:ee:ee:ee:ee:ee [ether]  on eth0
? (169.254.1.1) at ee:ee:ee:ee:ee:ee [ether]  on eth0
? (10.12.34.254) at 08:00:27:a7:59:11 [ether]  on net1
? (10.12.34.102) at 08:00:27:82:1a:95 [ether]  on net1
? (10.12.34.10) at 08:00:27:9b:a3:e4 [ether]  on net1
? (10.12.34.101) at <incomplete>  on net1


```

무작위 모드를 활성화 해도 privatezone에서는 외부로 통신이 안된다.
```
ping -I net1 -c 1 google.com
PING google.com (142.250.199.78) from 10.12.34.205 net1: 56(84) bytes of data.

--- google.com ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
```

pfsense에서는 privatezone IP 대역의 Source NAT를 제외 한 상태였다.
![image](https://user-images.githubusercontent.com/71689654/156815151-d670719a-e39c-4e6f-bb9f-568b0524a500.png)

